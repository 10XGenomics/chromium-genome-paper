{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating mid size deletions\n",
    "\n",
    "This notebook evaluates the results of the Long Ranger mid size deletion calls for NA12878.\n",
    "\n",
    "This notebook will not function as-is. In order to get it to run, you will need to have the [Comparative Annotation Toolkit](https://github.com/ComparativeGenomicsToolkit/Comparative-Annotation-Toolkit) installed to get access to the `tools` library. You will also need the SV analysis tool SURVIVOR, and access to Long Ranger VCFs as well as the Personalis truth set VCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.fileOps import *\n",
    "from tools.procOps import *\n",
    "import vcf\n",
    "import os\n",
    "from collections import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "def close_plot(pdf):\n",
    "    \"\"\"Convenience wrapper for producing PDFs\"\"\"\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    pdf.savefig(bbox_inches='tight')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACE YOUR LONG RANGER VCFS HERE.\n",
    "vcfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dels: avg len 848.530840214 num het: 2393 num_hom: 1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/ian.fiddes/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_svs: avg len nan num het: 0 num_hom: 0\n",
      "phased_variants: avg len 79.132127193 num het: 1135 num_hom: 689\n"
     ]
    }
   ],
   "source": [
    "# combine all VCFs and filter for passing deletions >50bp and <30kb\n",
    "c = Counter()  # keep track of provenance\n",
    "min_size = 50\n",
    "max_size = 30000\n",
    "def parse_gatk(f):\n",
    "    recs = [x for x in vcf.Reader(open(f)) if x.is_deletion and len(x.FILTER) == 0]\n",
    "    tmp_sizes = []\n",
    "    for rec in recs:\n",
    "        start, stop = rec._compute_coordinates_for_indel()\n",
    "        tmp_sizes.append(stop - start)\n",
    "        rec.INFO['SVLEN'] = stop - start\n",
    "        rec.INFO['SVTYPE'] = 'DEL'\n",
    "    return recs, tmp_sizes\n",
    "\n",
    "def parse_lr(f):\n",
    "    recs = [x for x in vcf.Reader(open(f)) if x.INFO['SVTYPE'] == 'DEL' and len(x.FILTER) == 0]\n",
    "    for x in recs:\n",
    "        x.INFO['SVLEN'] = abs(x.INFO['SVLEN'])\n",
    "    tmp_sizes = [x.INFO['SVLEN'] for x in recs]\n",
    "    return recs, tmp_sizes\n",
    "\n",
    "for f in vcfs:\n",
    "    fn = parse_gatk if 'phased_variants' in f else parse_lr\n",
    "    recs, tmp_sizes = fn(f)\n",
    "    filt = [[rec, s] for rec, s in zip(recs, tmp_sizes) if min_size <= s <= max_size]\n",
    "    t = os.path.basename(f).split('.')[0]\n",
    "    c[t] = len(filt_recs)\n",
    "    with open(t + '.filt.vcf', 'w') as outf:\n",
    "        outf_handle = vcf.Writer(outf, vcf.Reader(f))\n",
    "        for rec, s in filt:\n",
    "            outf_handle.write_record(rec)\n",
    "        outf_handle.close()\n",
    "    avg = np.nanmean([x[1] for x in filt])\n",
    "    num_het = len([x for x in filt if x[0].samples[0].is_het])\n",
    "    num_hom = len(filt) - num_het\n",
    "    print '{}: avg len {} num het: {} num_hom: {}'.format(t, avg, num_het, num_hom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging entries: 4118\n",
      "merging entries: 1824\n"
     ]
    }
   ],
   "source": [
    "# large SV file is empty\n",
    "!printf 'dels.filt.vcf\\nphased_variants.filt.vcf\\n' > fofn_tmp\n",
    "!SURVIVOR merge fofn_tmp 100 1 1 1 0 50 survivor_merged.vcf\n",
    "!rm fofn_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse SURVIVOR results\n",
    "\n",
    "def convert_call(call):\n",
    "    call = call.replace('|', '/')\n",
    "    if call == '0/1' or call == '1/0':\n",
    "        return 'het'\n",
    "    else:\n",
    "        return 'hom'\n",
    "\n",
    "results = []\n",
    "for l in open('survivor_merged.vcf'):\n",
    "    if not l.startswith('#'):\n",
    "        l = l.rstrip().split('\\t')\n",
    "        d = dict(x.split('=') for x in l[7].split(';') if x)\n",
    "        truth_supp, lr_supp = map(bool, map(int, list(d['SUPP_VEC'])))\n",
    "        dels_gt = convert_call(l[-2].split(':')[0])\n",
    "        gatk_gt = convert_call(l[-1].split(':')[0])\n",
    "        results.append([l[0], l[1], truth_supp, lr_supp, float(d['AVGLEN']), dels_gt, gatk_gt])\n",
    "survivor_df = pd.DataFrame(results, columns=['chrom', 'pos', 'lr_supp', 'gatk_supp', 'svlen', 'dels_gt', 'gatk_gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696.2259540498443"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survivor_df.svlen.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "het 2685\n",
      "hom 1829\n",
      "agree_het 330\n",
      "agree_hom 209\n",
      "disagree_het_hom 44\n",
      "disagree_hom_het 39\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "for _, s in survivor_df.iterrows():\n",
    "    if s.lr_supp and s.gatk_supp:\n",
    "        if s.dels_gt == s.gatk_gt:\n",
    "            c['agree_{}'.format(s.dels_gt)] += 1\n",
    "        else:\n",
    "            c['disagree_{}_{}'.format(s.dels_gt, s.gatk_gt)] += 1\n",
    "    elif s.lr_supp:\n",
    "        c[s.dels_gt] += 1\n",
    "    else:\n",
    "        c[s.gatk_gt] += 1\n",
    "for x, y in sorted(c.iteritems(), key=lambda x: -x[1]):\n",
    "    print x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "with open('gatk_lr_size_distribution_shared.pdf', 'w') as outf, PdfPages(outf) as pdf:\n",
    "    fig, ax = plt.subplots()\n",
    "    both = map(int, map(round, map(float, list(survivor_df[(survivor_df.gatk_supp == True) & (survivor_df.lr_supp == True)].svlen))))\n",
    "    gatk_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.gatk_supp == True) & (survivor_df.lr_supp == False)].svlen))))\n",
    "    lr_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.gatk_supp == False) & (survivor_df.lr_supp == True)].svlen))))\n",
    "    cutoff = 1000\n",
    "    both = [x for x in both if x <= cutoff]\n",
    "    lr_only = [x for x in lr_only if x <= cutoff]\n",
    "    gatk_only = [x for x in gatk_only if x <= cutoff]\n",
    "    ax.hist([both, gatk_only, lr_only], 200, histtype='stepfilled', label=['shared', 'GATK', 'dels'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_xlim((50, 1000))\n",
    "    ax.legend()\n",
    "    fig.suptitle('SURVIVOR merging of GATK and LongRanger deletions in 50-1000bp range')\n",
    "    close_plot(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'dels': 3445, 'gatk': 1069, 'shared': 622})\n"
     ]
    }
   ],
   "source": [
    "r = Counter()\n",
    "for _, s in survivor_df.iterrows():\n",
    "    if s.lr_supp and s.gatk_supp:\n",
    "        r['shared'] += 1\n",
    "    elif s.lr_supp:\n",
    "        r['dels'] += 1\n",
    "    else:\n",
    "        r['gatk'] += 1\n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try with Personalis/svclassify VCF\n",
    "# can be downloaded here also:\n",
    "# ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/technical/svclassify_Manuscript/Supplementary_Information/svclassify/svviz/input_vcf/\n",
    "vcfs = []\n",
    "with TemporaryDirectoryPath() as tmp_dir:\n",
    "    for v in vcfs:\n",
    "        !sed 's/ /\\t/g' {v} > {os.path.join(tmp_dir, os.path.basename(v))}\n",
    "    vcfs = glob(os.path.join(tmp_dir, '*'))\n",
    "    !vcf-concat {' '.join(vcfs)} > Personalis.hs37d5.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Personalis.hs37d5.filtered.vcf', 'w') as outf:\n",
    "    for l in open('Personalis.hs37d5.vcf'):\n",
    "        if l.startswith('#'):\n",
    "            outf.write(l)\n",
    "        else:\n",
    "            l = l.split('\\t')\n",
    "            l[0] = 'chr' + l[0]\n",
    "            d = dict(x.split('=') for x in l[-1].split(';'))\n",
    "            if 50 <= int(d['END']) - int(l[1]) <= 30000:\n",
    "                outf.write('\\t'.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging entries: 2294\n",
      "merging entries: 5136\n"
     ]
    }
   ],
   "source": [
    "!printf 'Personalis.hs37d5.filtered.vcf\\nsurvivor_merged.vcf\\n' > fofn_tmp\n",
    "!SURVIVOR merge fofn_tmp 100 1 1 1 0 50 svclassify_merged_dels_personalis.vcf\n",
    "!rm fofn_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 257 3109\n"
     ]
    }
   ],
   "source": [
    "# load the results\n",
    "results = []\n",
    "for l in open('svclassify_merged_dels_personalis.vcf'):\n",
    "    if not l.startswith('#'):\n",
    "        l = l.rstrip().split('\\t')\n",
    "        d = dict(x.split('=') for x in l[7].split(';') if x)\n",
    "        truth_supp, lr_supp = map(bool, map(int, list(d['SUPP_VEC'])))\n",
    "        results.append([l[0], l[1], truth_supp, lr_supp, d['AVGLEN']])\n",
    "survivor_df = pd.DataFrame(results, columns=['chrom', 'pos', 'truth_supp', 'lr_supp', 'svlen'])\n",
    "both = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == True) & (survivor_df.lr_supp == True)].svlen))))\n",
    "truth_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == True) & (survivor_df.lr_supp == False)].svlen))))\n",
    "lr_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == False) & (survivor_df.lr_supp == True)].svlen))))\n",
    "print len(both), len(truth_only), len(lr_only)\n",
    "\n",
    "# construct a triplet of plots showing 50-500bp and 500-10000\n",
    "with open('lr_truth_svclassify_size_distributions_personalis.pdf', 'w') as outf, PdfPages(outf) as pdf:\n",
    "    fig, axes = plt.subplots(ncols=2)\n",
    "    # 50-500bp\n",
    "    ax = axes[0]\n",
    "    ax.hist([[x for x in both if x <= 500], \n",
    "             [x for x in lr_only if x <= 500], \n",
    "             [x for x in truth_only if x <= 500]], \n",
    "            90, histtype='stepfilled', \n",
    "        label=['Clustered', 'LongRanger', 'Svclassify'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_title('50-500bp')\n",
    "    ax.set_xlim((50, 500))\n",
    "    # 500-5000bp\n",
    "    ax = axes[1]\n",
    "    ax.hist([[x for x in both if 500 <= x <= 10000], \n",
    "             [x for x in lr_only if 500 <= x <= 10000], \n",
    "             [x for x in truth_only if 500 <= x <= 10000]], \n",
    "            90, histtype='stepfilled', \n",
    "        label=['Clustered', 'LongRanger', 'Svclassify'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_title('500-10000bp')\n",
    "    ax.set_xlim((500, 10000))\n",
    "    ax.legend()\n",
    "    fig.suptitle('Size distributions of SURVIVOR clustering of LongRanger deletions with Svclassify truth set')\n",
    "    close_plot(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try lumpy\n",
    "bam = longranger_bam  # add this here!\n",
    "cmd = '''samtools sort -m 8G -@ 12 -O bam -T tmp.sort -n -o namesorted.bam {bam}\n",
    "'''\n",
    "run_proc(cmd.format(bam=bam).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd = [['samtools', 'view', '-h', 'namesorted.bam'],\n",
    "      ['samblaster', '--excludeDups', '--addMateTags', '--maxSplitCount', '2', '--minNonOverlap', '20'],\n",
    "      ['samtools', 'view', '-S', '-b', '-']]\n",
    "run_proc(cmd, stdout='sample.bam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools view -b -F 1294 sample.bam > sample.discordants.unsorted.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "samtools view -h sample.bam \\\n",
    "    | /mnt/home/ian.fiddes/lumpy-sv/scripts/extractSplitReads_BwaMem -i stdin \\\n",
    "    | samtools view -Sb - \\\n",
    "    > sample.splitters.unsorted.bam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sambamba sort sample.discordants.unsorted.bam sample.discordants\n",
    "!sambamba sort sample.splitters.unsorted.bam sample.splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_proc(['lumpyexpress', '-B', 'sample.bam', \n",
    "          '-S', 'sample.splitters.unsorted.sorted.bam', '-D', \n",
    "          'sample.discordants.unsorted.sorted.bam', '-o', 'sample.vcf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh = vcf.Reader(open('sample.vcf'))\n",
    "ofh = open('lumpy_dels.vcf', 'w')\n",
    "o = vcf.Writer(ofh, template=vh)\n",
    "for rec in vh:\n",
    "    if rec.INFO['SVTYPE'] == 'DEL' and 50 <= abs(rec.INFO['SVLEN'][0]) <= 30000:\n",
    "        o.write_record(rec)\n",
    "o.close()\n",
    "ofh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging entries: 2294\n",
      "merging entries: 19307\n"
     ]
    }
   ],
   "source": [
    "# repeat SURVIVOR analysis with lumpy data\n",
    "!printf 'Personalis.hs37d5.filtered.vcf\\nlumpy_dels.vcf\\n' > fofn_tmp\n",
    "!SURVIVOR merge fofn_tmp 100 1 1 1 0 50 svclassify_lumpy_merged.vcf\n",
    "!rm fofn_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263 1018 8307\n"
     ]
    }
   ],
   "source": [
    "# load the results\n",
    "results = []\n",
    "for l in open('svclassify_lumpy_merged.vcf'):\n",
    "    if not l.startswith('#'):\n",
    "        l = l.rstrip().split('\\t')\n",
    "        d = dict(x.split('=') for x in l[7].split(';') if x)\n",
    "        truth_supp, lr_supp = map(bool, map(int, list(d['SUPP_VEC'])))\n",
    "        results.append([l[0], l[1], truth_supp, lr_supp, d['AVGLEN']])\n",
    "survivor_df = pd.DataFrame(results, columns=['chrom', 'pos', 'truth_supp', 'lr_supp', 'svlen'])\n",
    "both = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == True) & (survivor_df.lr_supp == True)].svlen))))\n",
    "truth_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == True) & (survivor_df.lr_supp == False)].svlen))))\n",
    "lr_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == False) & (survivor_df.lr_supp == True)].svlen))))\n",
    "print len(both), len(truth_only), len(lr_only)\n",
    "\n",
    "# construct a triplet of plots showing 50-500bp and 500-10000\n",
    "with open('lumpy_truth_svclassify_size_distributions_personalis.pdf', 'w') as outf, PdfPages(outf) as pdf:\n",
    "    fig, axes = plt.subplots(ncols=2)\n",
    "    # 50-500bp\n",
    "    ax = axes[0]\n",
    "    ax.hist([[x for x in both if x <= 500], \n",
    "             [x for x in lr_only if x <= 500], \n",
    "             [x for x in truth_only if x <= 500]], \n",
    "            90, histtype='stepfilled', \n",
    "        label=['Clustered', 'Lumpy', 'Svclassify'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_title('50-500bp')\n",
    "    ax.set_xlim((50, 500))\n",
    "    # 500-5000bp\n",
    "    ax = axes[1]\n",
    "    ax.hist([[x for x in both if 500 <= x <= 10000], \n",
    "             [x for x in lr_only if 500 <= x <= 10000], \n",
    "             [x for x in truth_only if 500 <= x <= 10000]], \n",
    "            90, histtype='stepfilled', \n",
    "        label=['Clustered', 'Lumpy', 'Svclassify'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_title('500-10000bp')\n",
    "    ax.set_xlim((500, 10000))\n",
    "    ax.legend()\n",
    "    fig.suptitle('Size distributions of SURVIVOR clustering of Lumpy deletions with Svclassify truth set')\n",
    "    close_plot(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766.8432174858859"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt = list(vcf.Reader(open('lumpy_dels.vcf')))\n",
    "import numpy as np\n",
    "np.mean([abs(x.INFO['SVLEN'][0]) for x in filt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging entries: 2294\n",
      "merging entries: 4118\n",
      "2017 264 2048\n"
     ]
    }
   ],
   "source": [
    "# repeat for un-merged dels\n",
    "# large SV file is empty\n",
    "!printf 'Personalis.hs37d5.filtered.vcf\\ndels.filt.vcf\\n' > fofn_tmp\n",
    "!SURVIVOR merge fofn_tmp 100 1 1 1 0 50 svclassify_merged_dels_lr_only_personalis.vcf\n",
    "!rm fofn_tmp\n",
    "\n",
    "\n",
    "# load the results\n",
    "results = []\n",
    "for l in open('svclassify_merged_dels_lr_only_personalis.vcf'):\n",
    "    if not l.startswith('#'):\n",
    "        l = l.rstrip().split('\\t')\n",
    "        d = dict(x.split('=') for x in l[7].split(';') if x)\n",
    "        truth_supp, lr_supp = map(bool, map(int, list(d['SUPP_VEC'])))\n",
    "        results.append([l[0], l[1], truth_supp, lr_supp, d['AVGLEN']])\n",
    "survivor_df = pd.DataFrame(results, columns=['chrom', 'pos', 'truth_supp', 'lr_supp', 'svlen'])\n",
    "both = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == True) & (survivor_df.lr_supp == True)].svlen))))\n",
    "truth_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == True) & (survivor_df.lr_supp == False)].svlen))))\n",
    "lr_only = map(int, map(round, map(float, list(survivor_df[(survivor_df.truth_supp == False) & (survivor_df.lr_supp == True)].svlen))))\n",
    "print len(both), len(truth_only), len(lr_only)\n",
    "\n",
    "# construct a triplet of plots showing 50-500bp and 500-10000\n",
    "with open('lr_truth_dels_only_svclassify_size_distributions_personalis.pdf', 'w') as outf, PdfPages(outf) as pdf:\n",
    "    fig, axes = plt.subplots(ncols=2)\n",
    "    # 50-500bp\n",
    "    ax = axes[0]\n",
    "    ax.hist([[x for x in both if x <= 500], \n",
    "             [x for x in lr_only if x <= 500], \n",
    "             [x for x in truth_only if x <= 500]], \n",
    "            90, histtype='stepfilled', \n",
    "        label=['Clustered', 'LongRanger', 'Svclassify'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_title('50-500bp')\n",
    "    ax.set_xlim((50, 500))\n",
    "    # 500-5000bp\n",
    "    ax = axes[1]\n",
    "    ax.hist([[x for x in both if 500 <= x <= 10000], \n",
    "             [x for x in lr_only if 500 <= x <= 10000], \n",
    "             [x for x in truth_only if 500 <= x <= 10000]], \n",
    "            90, histtype='stepfilled', \n",
    "        label=['Clustered', 'LongRanger', 'Svclassify'], alpha=0.4)\n",
    "    ax.set_xlabel('Size of deletion')\n",
    "    ax.set_ylabel('Number of deletion calls')\n",
    "    ax.set_title('500-10000bp')\n",
    "    ax.set_xlim((500, 10000))\n",
    "    ax.legend()\n",
    "    fig.suptitle('Size distributions of SURVIVOR clustering of LongRanger deletions with Svclassify truth set')\n",
    "    close_plot(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699 2630\n"
     ]
    }
   ],
   "source": [
    "recs = [x.split() for x in open('svclassify_merged_dels_lr_only_personalis.vcf') if not x.startswith('#')]\n",
    "hom = [x for x in recs if x[-1].startswith('1/1') or x[-1].startswith('1|1')]\n",
    "print len(hom), len(recs) - len(hom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
